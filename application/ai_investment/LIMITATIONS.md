# AI Investment Data: Known Limitations

## Critical Issues for Boundary Detection

### 1. Insufficient Time Series
- **Problem**: Only 8 years of data (2015-2023)
- **Impact**: Cannot observe full regime dynamics
- **Consequence**: "Boundaries" may be noise, not real regime changes

### 2. COVID-19 Confounding
- **Problem**: Pandemic (2020-2022) creates massive disruption
- **Impact**: Cannot separate AI effects from pandemic effects
- **Consequence**: Any detected "boundary crossing" may be pandemic, not AI

### 3. Measurement Error
- **Problem**: "AI adoption" is poorly defined
- **Examples**: Chatbot? Machine learning? Automation? RPA?
- **Impact**: Treatment variable has high error
- **Consequence**: Attenuates boundary detection

### 4. Selection Bias
- **Problem**: Early AI adopters are different firms
- **Cannot control for**: Unmeasured innovation capacity
- **Consequence**: Confounds boundary crossing with pre-existing differences

### 5. Theory Validation Paradox
- **Problem**: Using same data to develop AND validate theory
- **This violates**: Scientific method of independent validation
- **Consequence**: Circular reasoning, overfitting

## Appropriate Use of This Data

**DO**: Use as exploratory application after validation on China shock
**DON'T**: Use to validate the boundary detection methodology

## Honest Research Framing

When writing about AI investment results:
- Frame as "exploratory application"
- Note all five limitations explicitly
- Compare with validated method from China shock
- Acknowledge uncertainty in findings
- Provide sensitivity analyses

## Alternative if Validation Fails

If China shock validation fails, DO NOT proceed with AI application.
Instead: Refine method, validate again, then apply to AI.
